{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient \n",
    "bc = BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from setup import get_train, get_test, get_valid\n",
    "train = get_train(6)\n",
    "test = get_test(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/liar/train_whead.csv')\n",
    "test.to_csv('data/liartest_whead.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10240"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_str_train = train['statement']\n",
    "y_train = train['label']\n",
    "X_str_test = test['statement']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-as-a-service recommends not pre-batching the inputs, since the program does this automatically\n",
    "#however, despite reading \"sent back size 10240\" after the job completes, the jupyter cell still hangs\n",
    "#batching prevents this issue, and lets the BERT encoding run to completion successfully\n",
    "ys = []\n",
    "n = len(X_str_train)\n",
    "for i in range(n):\n",
    "    y, _ = bc.encode(\n",
    "        X_str_train.tolist()[i:(i+1)], show_tokens=True)\n",
    "    ys.append(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually pad and concatenate the sentence by sentence encodings\n",
    "m = max(y.shape[0] for y in ys)\n",
    "X_bert_train = np.zeros((n, m, 768))\n",
    "for i in range(len(ys)):\n",
    "    y = ys[i]\n",
    "    X_bert_train[i] = np.pad(y, ((0, m-y.shape[0]), (0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/bert/X_bert_train', X_bert_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Bert_train = np.load('data/bert/X_bert_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average the sentence level encodings into a single 768-dimension vector\n",
    "def bert_reduce_mean(X):\n",
    "    return X.mean(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train_mean = bert_reduce_mean(X_bert_train)\n",
    "np.save('data/bert/X_bert_train_mean', X_bert_train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = TorchShallowNeuralClassifier(\n",
    "    max_iter=110, hidden_dim=500)\n",
    "#changing learning rate (eta) and \n",
    "#l2 regularization (l2_strength) suboptimum \n",
    "#for this model space at parameters other than default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 110 of 110; error is 14.775890350341797"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 23s, sys: 1min 14s, total: 5min 37s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = mod.fit(X_bert_train_mean, tuple(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys2 = []\n",
    "n2 = len(X_str_test)\n",
    "for i in range(n2):\n",
    "    y, _ = bc.encode(\n",
    "        X_str_test.tolist()[i:(i+1)], show_tokens=True)\n",
    "    ys2.append(y[0])\n",
    "#perform encoding for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = max(y.shape[0] for y in ys)\n",
    "X_bert_test = np.zeros((n2, m2, 768))\n",
    "for i in range(len(ys2)):\n",
    "    y = ys2[i]\n",
    "    X_bert_test[i] = np.pad(y, ((0, m2-y.shape[0]), (0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/bert/X_bert_test', X_bert_test)\n",
    "#X_bert_test = np.load('data/bert/X_bert_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_test_mean = bert_reduce_mean(X_bert_test)\n",
    "np.save('data/bert/X_bert_test_mean', X_bert_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_preds = mod.predict(X_bert_train_mean)\n",
    "print(classification_report(y_train, bert_train_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE      0.308     0.265     0.285       249\n",
      "        TRUE      0.240     0.231     0.235       208\n",
      " barely-true      0.260     0.118     0.162       212\n",
      "   half-true      0.253     0.321     0.283       265\n",
      " mostly-true      0.225     0.324     0.265       241\n",
      "  pants-fire      0.270     0.217     0.241        92\n",
      "\n",
      "   micro avg      0.254     0.254     0.254      1267\n",
      "   macro avg      0.259     0.246     0.245      1267\n",
      "weighted avg      0.259     0.254     0.249      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_test_preds = mod.predict(X_bert_test_mean)\n",
    "print(classification_report(y_test, bert_test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_train = pd.read_csv('liwc_train.csv')\n",
    "#pre-computed LIWC-2015 corpus feature-level counts on each statement. \n",
    "#Extends the columns by ~100 features (in addition to meta-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend the BERT feature vectors with the LIWC features\n",
    "X_bert_train_liwc = np.ndarray((extended_train.shape[0], 861))\n",
    "for j in range(extended_train.shape[0]):\n",
    "    X_bert_train_liwc[j] = np.append(X_bert_train_mean[j], extended_train.loc[:,'WC':'OtherP'].iloc[j].values)\n",
    "np.save('data/bert/X_bert_train_liwc', X_bert_train_liwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_test = pd.read_csv('liwc_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_test_liwc = np.ndarray((extended_test.shape[0], 861))\n",
    "for j in range(extended_test.shape[0]):\n",
    "    X_bert_test_liwc[j] = np.append(X_bert_test_mean[j], extended_train.loc[:,'WC':'OtherP'].iloc[j].values)\n",
    "np.save('data/bert/X_bert_test_liwc', X_bert_test_liwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend only with the three feature functions of most interest from the Pennebaker\n",
    "X_bert_train_liwc_rest = np.ndarray((extended_train.shape[0], 771))\n",
    "for j in range(extended_train.shape[0]):\n",
    "    X_bert_train_liwc_rest[j] = np.append(X_bert_train_mean[j], extended_train[['ppron', 'negemo', 'cogproc']].values[j])\n",
    "np.save('data/bert/X_bert_train_liwc_rest', X_bert_train_liwc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_test_liwc_rest = np.ndarray((extended_test.shape[0], 771))\n",
    "for j in range(extended_test.shape[0]):\n",
    "    X_bert_test_liwc_rest[j] = np.append(X_bert_test_mean[j], extended_test[['ppron', 'negemo', 'cogproc']].values[j])\n",
    "np.save('data/bert/X_bert_test_liwc_rest', X_bert_test_liwc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = TorchShallowNeuralClassifier(\n",
    "    max_iter=100, hidden_dim=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 14.09523606300354"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 29s, sys: 1min 2s, total: 6min 31s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = mod1.fit(X_bert_train_liwc_rest, tuple(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_preds = mod1.predict(X_bert_test_liwc_rest)\n",
    "bert_train_preds = mod1.predict(X_bert_train_liwc_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE      0.317     0.237     0.271       249\n",
      "        TRUE      0.229     0.260     0.243       208\n",
      " barely-true      0.247     0.208     0.226       212\n",
      "   half-true      0.271     0.309     0.289       265\n",
      " mostly-true      0.238     0.224     0.231       241\n",
      "  pants-fire      0.234     0.348     0.279        92\n",
      "\n",
      "   micro avg      0.257     0.257     0.257      1267\n",
      "   macro avg      0.256     0.264     0.257      1267\n",
      "weighted avg      0.260     0.257     0.256      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, bert_test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE      0.466     0.364     0.409      1995\n",
      "        TRUE      0.449     0.479     0.463      1676\n",
      " barely-true      0.459     0.383     0.418      1654\n",
      "   half-true      0.433     0.509     0.468      2114\n",
      " mostly-true      0.470     0.479     0.475      1962\n",
      "  pants-fire      0.437     0.533     0.480       839\n",
      "\n",
      "   micro avg      0.452     0.452     0.452     10240\n",
      "   macro avg      0.452     0.458     0.452     10240\n",
      "weighted avg      0.454     0.452     0.450     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, bert_train_preds, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
