{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient \n",
    "bc = BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from setup import get_train, get_test, get_valid\n",
    "train6 = get_train(6)\n",
    "test6 = get_test(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = get_train(3)\n",
    "test3 = get_test(3)\n",
    "train2 = get_train(2)\n",
    "test2 = get_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str_train = train6['statement']\n",
    "y_train6 = train6['label']\n",
    "X_str_test = test6['statement']\n",
    "y_test6 = test6['label']\n",
    "\n",
    "y_train3 = get_train(3)['label']\n",
    "y_test3 = get_test(3)['label']\n",
    "y_train2 = get_train(2)['label']\n",
    "y_test2 = get_test(2)['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-as-a-service recommends not pre-batching the inputs, since the program does this automatically\n",
    "#however, despite reading \"sent back size 10240\" after the job completes, the jupyter cell still hangs\n",
    "#batching prevents this issue, and lets the BERT encoding run to completion successfully\n",
    "ys = []\n",
    "n = len(X_str_train)\n",
    "for i in range(n):\n",
    "    y, _ = bc.encode(\n",
    "        X_str_train.tolist()[i:(i+1)], show_tokens=True)\n",
    "    ys.append(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually pad and concatenate the sentence by sentence encodings\n",
    "m = max(y.shape[0] for y in ys)\n",
    "X_bert_train = np.zeros((n, m, 768))\n",
    "for i in range(len(ys)):\n",
    "    y = ys[i]\n",
    "    X_bert_train[i] = np.pad(y, ((0, m-y.shape[0]), (0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/bert/X_bert_train', X_bert_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Bert_train = np.load('data/bert/X_bert_train.npy')\n",
    "#remove pre-mean bert_train to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average the sentence level encodings into a single 768-dimension vector\n",
    "def bert_reduce_mean(X):\n",
    "    return X.mean(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_bert_train_mean = bert_reduce_mean(X_bert_train)\n",
    "#np.save('data/bert/X_bert_train_mean', X_bert_train_mean)\n",
    "X_bert_train_mean = np.load('data/bert/X_bert_train_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = TorchShallowNeuralClassifier(\n",
    "    max_iter=110, hidden_dim=500)\n",
    "#changing learning rate (eta) and \n",
    "#l2 regularization (l2_strength) suboptimum \n",
    "#for this model space at parameters other than default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 110 of 110; error is 14.775890350341797"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 23s, sys: 1min 14s, total: 5min 37s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train6\n",
    "%time _ = mod1.fit(X_bert_train_mean, tuple(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys2 = []\n",
    "n2 = len(X_str_test)\n",
    "for i in range(n2):\n",
    "    y, _ = bc.encode(\n",
    "        X_str_test.tolist()[i:(i+1)], show_tokens=True)\n",
    "    ys2.append(y[0])\n",
    "#perform encoding for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = max(y.shape[0] for y in ys)\n",
    "X_bert_test = np.zeros((n2, m2, 768))\n",
    "for i in range(len(ys2)):\n",
    "    y = ys2[i]\n",
    "    X_bert_test[i] = np.pad(y, ((0, m2-y.shape[0]), (0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/bert/X_bert_test', X_bert_test)\n",
    "#X_bert_test = np.load('data/bert/X_bert_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_bert_test_mean = bert_reduce_mean(X_bert_test)\n",
    "#np.save('data/bert/X_bert_test_mean', X_bert_test_mean)\n",
    "X_bert_test_mean = np.load('data/bert/X_bert_test_mean.npy')\n",
    "X_bert_train_mean = np.load('data/bert/X_bert_train_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10240, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bert_train_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_preds = mod.predict(X_bert_train_mean)\n",
    "print(classification_report(y_train, bert_train_preds, digits=3))\n",
    "bert_test_preds = mod.predict(X_bert_test_mean)\n",
    "print(classification_report(y_test, bert_test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_train = pd.read_csv('data/liar/liwc_train.csv')\n",
    "extended_test = pd.read_csv('data/liar/liwc_test.csv')\n",
    "#pre-computed LIWC-2015 corpus feature-level counts on each statement. \n",
    "#Extends the columns by ~100 features (in addition to meta-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_train(subset):\n",
    "    train_e = np.ndarray((extended_train.shape[0], X_bert_train_mean.shape[1]+len(subset)))\n",
    "    for j in range(extended_train.shape[0]):\n",
    "        train_e[j] = np.append(X_bert_train_mean[j], extended_train[subset].values[j])\n",
    "    return train_e\n",
    "\n",
    "def extend_test(subset):\n",
    "    test_e = np.ndarray((extended_test.shape[0], X_bert_test_mean.shape[1]+len(subset)))\n",
    "    for j in range(extended_test.shape[0]):\n",
    "        test_e[j] = np.append(X_bert_test_mean[j], extended_test[subset].values[j])\n",
    "    return test_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_liwc_labels = extended_train.columns[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend the BERT feature vectors with the LIWC features\n",
    "#X_bert_train_liwc = extend_train(all_liwc_labels)\n",
    "#np.save('data/bert/X_bert_train_liwc', X_bert_train_liwc)\n",
    "#X_bert_test_liwc = extend_test(all_liwc_labels)\n",
    "#np.save('data/bert/X_bert_test_liwc', X_bert_test_liwc)\n",
    "X_bert_test_liwc = np.load('data/bert/X_bert_test_liwc.npy')\n",
    "X_bert_train_liwc = np.load('data/bert/X_bert_train_liwc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend only with the three feature functions of interest from Newman et al.\n",
    "subset = ['ppron', 'negemo', 'cogproc']\n",
    "#X_bert_train_liwc_rest = extend_train(subset)\n",
    "#np.save('data/bert/X_bert_train_liwc_rest', X_bert_train_liwc_rest)\n",
    "#X_bert_test_liwc_rest = extend_test(subset)\n",
    "#np.save('data/bert/X_bert_test_liwc_rest', X_bert_test_liwc_rest)\n",
    "X_bert_train_liwc_rest = np.load('data/bert/X_bert_train_liwc_rest.npy')\n",
    "X_bert_test_liwc_rest = extend_test('data/bert/X_bert_test_liwc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend with a random subset of 10 feature functions\n",
    "chosen = 10\n",
    "subset = np.random.choice(extended_train.columns[15:], chosen)\n",
    "\n",
    "X_bert_train_liwc_rand = extend_train(subset)\n",
    "X_bert_test_liwc_rand = extend_test(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = TorchShallowNeuralClassifier(\n",
    "    max_iter=100, hidden_dim=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 7.013440847396851"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 35s, sys: 35.7 s, total: 4min 11s\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "#set the training set to be the extension with featureset of choice, and choose y as desired (number of labels)\n",
    "train_wbl = X_bert_train_liwc_rand\n",
    "test_wbl = X_bert_test_liwc_rand\n",
    "y_train = y_train6\n",
    "%time _ = mod1.fit(train_wbl, tuple(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_preds = mod1.predict(train_wbl)\n",
    "bert_test_preds = mod1.predict(test_wbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.903     0.746     0.817       839\n",
      "           1      0.838     0.751     0.792      1995\n",
      "           2      0.842     0.747     0.792      1654\n",
      "           3      0.852     0.750     0.797      2114\n",
      "           4      0.846     0.696     0.764      1962\n",
      "           5      0.553     0.930     0.694      1676\n",
      "\n",
      "   micro avg      0.768     0.768     0.768     10240\n",
      "   macro avg      0.806     0.770     0.776     10240\n",
      "weighted avg      0.802     0.768     0.774     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.059     0.011     0.018        92\n",
      "           1      0.216     0.301     0.252       249\n",
      "           2      0.135     0.160     0.147       212\n",
      "           3      0.216     0.468     0.295       265\n",
      "           4      0.158     0.012     0.023       241\n",
      "           5      0.190     0.053     0.083       208\n",
      "\n",
      "   micro avg      0.196     0.196     0.196      1267\n",
      "   macro avg      0.162     0.168     0.136      1267\n",
      "weighted avg      0.176     0.196     0.155      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, bert_train_preds, digits=3))\n",
    "print(classification_report(y_test, bert_test_preds, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
