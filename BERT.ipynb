{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient \n",
    "bc = BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from setup import get_train, get_test, get_valid\n",
    "train6 = get_train(6)\n",
    "test6 = get_test(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = get_train(3)\n",
    "test3 = get_test(3)\n",
    "train2 = get_train(2)\n",
    "test2 = get_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str_train = train6['statement']\n",
    "y_train6 = train6['label']\n",
    "X_str_test = test6['statement']\n",
    "y_test6 = test6['label']\n",
    "\n",
    "y_train3 = get_train(3)['label']\n",
    "y_test3 = get_test(3)['label']\n",
    "y_train2 = get_train(2)['label']\n",
    "y_test2 = get_test(2)['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-as-a-service recommends not pre-batching the inputs, since the program does this automatically\n",
    "#however, despite reading \"sent back size 10240\" after the job completes, the jupyter cell still hangs\n",
    "#batching prevents this issue, and lets the BERT encoding run to completion successfully\n",
    "ys = []\n",
    "n = len(X_str_train)\n",
    "for i in range(n):\n",
    "    y, _ = bc.encode(\n",
    "        X_str_train.tolist()[i:(i+1)], show_tokens=True)\n",
    "    ys.append(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually pad and concatenate the sentence by sentence encodings\n",
    "m = max(y.shape[0] for y in ys)\n",
    "X_bert_train = np.zeros((n, m, 768))\n",
    "for i in range(len(ys)):\n",
    "    y = ys[i]\n",
    "    X_bert_train[i] = np.pad(y, ((0, m-y.shape[0]), (0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/bert/X_bert_train', X_bert_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Bert_train = np.load('data/bert/X_bert_train.npy')\n",
    "#remove pre-mean bert_train to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average the sentence level encodings into a single 768-dimension vector\n",
    "def bert_reduce_mean(X):\n",
    "    return X.mean(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_bert_train_mean = bert_reduce_mean(X_bert_train)\n",
    "#np.save('data/bert/X_bert_train_mean', X_bert_train_mean)\n",
    "X_bert_train_mean = np.load('data/bert/X_bert_train_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_deep_neural_classifier import TorchDeepNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = TorchShallowNeuralClassifier(\n",
    "    max_iter=110, hidden_dim=500)\n",
    "#changing learning rate (eta) and \n",
    "#l2 regularization (l2_strength) suboptimum \n",
    "#for this model space at parameters other than default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 110 of 110; error is 14.645215868949893"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 45s, sys: 43.2 s, total: 4min 28s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train2\n",
    "%time _ = mod1.fit(X_bert_train_mean, tuple(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys2 = []\n",
    "n2 = len(X_str_test)\n",
    "for i in range(n2):\n",
    "    y, _ = bc.encode(\n",
    "        X_str_test.tolist()[i:(i+1)], show_tokens=True)\n",
    "    ys2.append(y[0])\n",
    "#perform encoding for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = max(y.shape[0] for y in ys)\n",
    "X_bert_test = np.zeros((n2, m2, 768))\n",
    "for i in range(len(ys2)):\n",
    "    y = ys2[i]\n",
    "    X_bert_test[i] = np.pad(y, ((0, m2-y.shape[0]), (0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/bert/X_bert_test', X_bert_test)\n",
    "#X_bert_test = np.load('data/bert/X_bert_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_bert_test_mean = bert_reduce_mean(X_bert_test)\n",
    "#np.save('data/bert/X_bert_test_mean', X_bert_test_mean)\n",
    "#X_bert_test_mean = np.load('data/bert/X_bert_test_mean.npy')\n",
    "#X_bert_train_mean = np.load('data/bert/X_bert_train_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_preds = mod1.predict(X_bert_train_mean)\n",
    "bert_test_preds = mod1.predict(X_bert_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.420     0.464     0.441       839\n",
      "           1      0.356     0.575     0.440      1995\n",
      "           2      0.519     0.190     0.279      1654\n",
      "           3      0.405     0.462     0.432      2114\n",
      "           4      0.453     0.394     0.421      1962\n",
      "           5      0.454     0.368     0.406      1676\n",
      "\n",
      "   micro avg      0.412     0.412     0.412     10240\n",
      "   macro avg      0.434     0.409     0.403     10240\n",
      "weighted avg      0.432     0.412     0.403     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.248     0.326     0.282        92\n",
      "           1      0.285     0.414     0.337       249\n",
      "           2      0.263     0.118     0.163       212\n",
      "           3      0.250     0.291     0.269       265\n",
      "           4      0.258     0.224     0.240       241\n",
      "           5      0.256     0.212     0.232       208\n",
      "\n",
      "   micro avg      0.263     0.263     0.263      1267\n",
      "   macro avg      0.260     0.264     0.254      1267\n",
      "weighted avg      0.261     0.263     0.254      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, bert_train_preds, digits=3))\n",
    "print(classification_report(y_test, bert_test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_train = pd.read_csv('data/liar/liwc_train.csv')\n",
    "extended_test = pd.read_csv('data/liar/liwc_test.csv')\n",
    "#pre-computed LIWC-2015 corpus feature-level counts on each statement. \n",
    "#Extends the columns by ~100 features (in addition to meta-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_train(subset):\n",
    "    train_e = np.ndarray((extended_train.shape[0], X_bert_train_mean.shape[1]+len(subset)))\n",
    "    for j in range(extended_train.shape[0]):\n",
    "        train_e[j] = np.append(X_bert_train_mean[j], extended_train[subset].values[j])\n",
    "    return train_e\n",
    "\n",
    "def extend_test(subset):\n",
    "    test_e = np.ndarray((extended_test.shape[0], X_bert_test_mean.shape[1]+len(subset)))\n",
    "    for j in range(extended_test.shape[0]):\n",
    "        test_e[j] = np.append(X_bert_test_mean[j], extended_test[subset].values[j])\n",
    "    return test_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_liwc_labels = extended_train.columns[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend the BERT feature vectors with the LIWC features\n",
    "#X_bert_train_liwc = extend_train(all_liwc_labels)\n",
    "#np.save('data/bert/X_bert_train_liwc', X_bert_train_liwc)\n",
    "#X_bert_test_liwc = extend_test(all_liwc_labels)\n",
    "#np.save('data/bert/X_bert_test_liwc', X_bert_test_liwc)\n",
    "X_bert_test_liwc = np.load('data/bert/X_bert_test_liwc.npy')\n",
    "X_bert_train_liwc = np.load('data/bert/X_bert_train_liwc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend only with the three feature functions of interest from Newman et al.\n",
    "subset = ['ppron', 'negemo', 'cogproc']\n",
    "#X_bert_train_liwc_rest = extend_train(subset)\n",
    "#np.save('data/bert/X_bert_train_liwc_rest', X_bert_train_liwc_rest)\n",
    "#X_bert_test_liwc_rest = extend_test(subset)\n",
    "#np.save('data/bert/X_bert_test_liwc_rest', X_bert_test_liwc_rest)\n",
    "X_bert_train_liwc_rest = np.load('data/bert/X_bert_train_liwc_rest.npy')\n",
    "X_bert_test_liwc_rest = extend_test('data/bert/X_bert_test_liwc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend with a random subset of 10 feature functions\n",
    "chosen = 10\n",
    "subset = np.random.choice(extended_train.columns[15:], chosen)\n",
    "\n",
    "X_bert_train_liwc_rand = extend_train(subset)\n",
    "X_bert_test_liwc_rand = extend_test(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = TorchShallowNeuralClassifier(\n",
    "    max_iter=100, hidden_dim=500)\n",
    "\n",
    "#mod3 = TorchDeepNeuralClassifier(\n",
    "#    max_iter=100, hidden_dim=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 5.217316746711731"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 22s, sys: 46.4 s, total: 4min 8s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "#set the training set to be the extension with featureset of choice, and choose y as desired (number of labels)\n",
    "train_wbl = X_bert_train_liwc_rand\n",
    "test_wbl = X_bert_test_liwc_rand\n",
    "y_train = y_train2\n",
    "%time _ = mod2.fit(train_wbl, tuple(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_preds = mod2.predict(train_wbl)\n",
    "bert_test_preds = mod2.predict(test_wbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.891     0.433     0.583      4488\n",
      "           1      0.684     0.959     0.799      5752\n",
      "\n",
      "   micro avg      0.728     0.728     0.728     10240\n",
      "   macro avg      0.788     0.696     0.691     10240\n",
      "weighted avg      0.775     0.728     0.704     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.278     0.384       553\n",
      "           1      0.608     0.867     0.715       714\n",
      "\n",
      "   micro avg      0.610     0.610     0.610      1267\n",
      "   macro avg      0.613     0.573     0.549      1267\n",
      "weighted avg      0.613     0.610     0.570      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train2, bert_train_preds, digits=3))\n",
    "print(classification_report(y_test2, bert_test_preds, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
